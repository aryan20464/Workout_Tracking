---
title: "Project Report"
author: "Chakravarthy Gajvelly"
date: "23 October 2015"
output: html_document
---

<h2> Introduction to the project:</h2>
<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.
</p>
<p>
In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and try to predict the manner of how the excercise is done.</p>
<h2>1. Preprocessing the data</h2>
<h3>1.1 Packages used : </h3>
```{r}
library(ggplot2)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(corrplot) 
```
<h3>1.2 Details of the data set used:</h3>
<p>The data is taken from the links obtained from the project excercise page and are downloaded and placed in the data folder of the project directory. The .csv files are read locally from this location for conducting this study.</p>
```{r}
data_train<-read.csv("/home/chakri/Model/data/pml-training.csv")
data_test<-read.csv("/home/chakri/Model/data/pml-testing.csv")
dim(data_train)
dim(data_test)
```
<p> The dimensions of the training and test data sets are shown above. Both of them have different number of entries but the common for them is the 160 columns. Training data has 19622 entries(rows) and testing data has 20 entries(rows). Training data has a column "classe", it is the dependent variable in this case which provides the result of the project.</p>

<h3>1.3 Cleaning the data</h3>
<p> The training and testing data obtained consists of some columns that have missing values(NA). so those columns are not necessary for the study. We therefore refine the both training and testing data sets using "colSums" function. If the output of function on a given column is zero then the column contains only NA's. Some of the columns are removed in this process. The size of new data has less columns than original data. It is done as shown below: </p>
```{r}
data_train<-data_train[,colSums(is.na(data_train))==0]
data_test<-data_test[,colSums(is.na(data_test))==0]
dim(data_train)
dim(data_test)
```

<p>Classe is the predictor in our case which provdies result of the study. Some of the attributes or columns doesnot provide any information or they are useless for the outcome varibale "classe", they must be removed for obtaining flexible data. Iam using grepl function for this purpose, it matches the string/regular expression in the first argument upon the data given in second argument.</p>

<p> The attributes in the training and testing data are shown below:</p>
```{r}
names(data_train)
names(data_test)
```
<p>
The training data is hence removed all the unnecessary attributes. It is shown below:
</p
```{r}
classe <- data_train$classe
trainRemove <- grepl("^X|timestamp|window", names(data_train))
data_train <- data_train[, !trainRemove]
train_cleaned <- data_train[, sapply(data_train, is.numeric)]
train_cleaned$classe <- classe
dim(train_cleaned)
```
<p>
The testing data is hence removed all the unnecessary attributes in the above manner as well. It is shown below:
</p-<h3>3. Cleaning the data</h3>

```{r}
testRemove <- grepl("^X|timestamp|window", names(data_test))
data_test <- data_test[, !testRemove]
testCleaned <- data_test[, sapply(data_test, is.numeric)]
dim(testCleaned)
```

<h3>1.4 Data partition for training and testing the model</h3>
```{r}
data_partition <- createDataPartition(train_cleaned$classe, p=0.70, list=F)
model_train <- train_cleaned[data_partition, ]
testData <- train_cleaned[-data_partition, ]
dim(model_train)
dim(testData)
```

<h2> 2. Creating the model using Random Forests </h2>
<p>I have fitted the model using Random Forest algorithm as it preselects the attributes or independent variables that are best for predicting the outcome accurately, its flexibility  in correlated covariates and other general factors in decision trees. I have used a 5-fold cross validation before applying the algorithm using trainControl function for controlRf attribute of train method in caret package. The rest of the functionality is shown below </p>
```{r}
controlRf <- trainControl(method="cv", 5)
controlRf
gen_model <- train(classe ~ ., data=model_train, method="rf", trControl=controlRf, ntree=100)
gen_model
```
<p> The above output shows the summary of sample sizes which are 5 in number as we chose 5 fold cross validation.</p>
<h4>2.1 Predicting the results using the model generated</h4>
```{r}
gen_prediction <- predict(gen_model, testData)
confusionMatrix(testData$classe, gen_prediction)
```
<p> In the above output the section "Overall Statistics" has some values for outputs, Accuracy and the out of sample error estimate is shown below.
</p>
```{r}
Accuracy <- postResample(gen_prediction, testData$classe)
Accuracy
out_of_sample_error <- 1 - as.numeric(confusionMatrix(testData$classe, gen_prediction)$overall[1])
out_of_sample_error
```
<h4>2.2 Predicting the results of given test data </h4>
<p> The model is now used for predicting the "classe" or output of the given test data obtained from the assignment page</p>
```{r}
result<-predict(gen_model,testCleaned[,-length(names(testCleaned))])
result
```
<p>3. Plots:</p>
<p>3.1 Correlative Matrix Plot</p>
<p>This matrix shows the attributes present in the data and how far are they concerned to decision making or predicting the outcome.</p>
```{r fig.width=12, fig.height=15}
classPlot <- cor(model_train[, -length(names(model_train))])
corrplot(classPlot)
```
<p>3.2 Tree Diagram</p>
<p>It shows the intervals below and above which the classification is made using decision trees, the entropies are calculated for this purpose but are not necessarily shown in the output</p>
```{r fig.width=12, fig.height=15}
decision_gen_Tree <- rpart(classe ~ ., data=model_train, method="class")
prp(decision_gen_Tree)
```